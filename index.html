
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"> 
    <title>Amanda Coston</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.3.3/semantic.min.css">
</head>
<body>

<div class="ui secondary pointing menu">
  <a class="active item">
    Home
  </a>
  <a class="item" href="research.html">
    Research
  </a>
  <a class="item" href="about.html">
    About
  </a>
  <a class="item" href="cv.pdf">
    CV
  </a>
</div>

<div class="ui items">
  <div class="item">
    <div class="image">
      <img src="pic.jpg">
    </div>
    <div class="content">
      <div class="description">
        <p>Amanda Coston is a PhD student in <a href="https://www.ml.cmu.edu/current-students/joint-phd-in-machine-learning-and-public-policy-requirements.html">Machine Learning and Public Policy</a> at Carnegie Mellon University (CMU). Her <a href="research.html">research</a> considers the impact of algorithmic risk assessments in settings such as child welfare screening, criminal justice, and loan approvals. She is particularly interested in how techniques from causal inference and transfer learning can resolve current limitations of these systems.  She is advised by <a href= "https://www.andrew.cmu.edu/user/achoulde/">Alexandra Chouldechova</a> and <a href="http://www.ehkennedy.com/">Edward H. Kennedy</a>. 
<br> 
      <h2>Featured Research</h2>
       <b> A. Coston, </b> N. Guha, L. Lu, D. Ouyang, A. Chouldechova, and D. Ho. "Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy." <i> ACM Conference on Fairness, Accountability, and Transparency</i>, 2021. <a href = "https://dl.acm.org/doi/10.1145/3442188.3445881">[Paper]</a> <a href="https://arxiv.org/abs/2011.07194">[ArXiv]</a>  <a href="https://www.youtube.com/watch?v=K2axD2sv-Uc">[Talk]</a>
      <br>
        <br>

        <b>A. Coston</b>, A. Rambachan, and A. Chouldechova. "Characterizing Fairness over the Set of Good Models Under Selective Labels." <i>International Conference on Machine Learning</i>, 2021. <a href="https://proceedings.mlr.press/v139/coston21a.html">[Paper]</a>  <a href="https://arxiv.org/abs/2101.00352">[ArXiv]</a> 
        <br>
        <br>

        <b>A. Coston</b>, E. H. Kennedy, and A. Chouldechova. "Counterfactual Predictions under Runtime Confounding." <i>Neural Information Processing Systems</i>, 2020. <a href = "https://papers.nips.cc/paper/2020/hash/2b64c2f19d868305aa8bbc2d72902cc5-Abstract.html">[Paper]</a> <a href="https://arxiv.org/abs/2006.16916">[ArXiv]</a> <a href ="https://blog.ml.cmu.edu/2021/04/16/counterfactual-predictions-under-runtime-confounding/"> [Blog] </a> 
        <br>
        <br>

        <b>A. Coston</b>, A. Mishler, E. H. Kennedy, and A. Chouldechova. "Counterfactual Risk Assessments, Evaluation, and Fairness." ACM FAT* 2020. <a href = "https://dl.acm.org/doi/abs/10.1145/3351095.3372851">[Paper]</a> <a href="https://arxiv.org/pdf/1909.00066.pdf">[ArXiv]</a> <a href="https://www.youtube.com/watch?v=9zfi3heBYUs">[Talk]</a>
        <br>
        <br>

        <b>A. Coston</b>, K. N. Ramamurthy, D. Wei, K. R. Varshney, S. Speakman, Z. Mustahsan, S. Chakraborty.  "Fair Transfer Learning with Missing Protected Attributes," <i> AAAI/ACM Conference on Artificial Intellligence, Ethics, and Society (AIES)</i>, 2019. <a href="https://dl.acm.org/doi/10.1145/3306618.3314236">[Paper]</a>
         <br>
        <br>
      
<h2>News</h2>
<ul>
<li><i>June 7, 2021</i> Starting internship at Facebook Responsible AI</li>
<li><i>May 18, 2021</i>  Featured on <a href="https://www.placekey.io/blog/amanda-coston-carnegie-mellon">Placekey Spotlight</a> </li>
  <li><i>May 8, 2021</i> Research on <a href="https://arxiv.org/abs/2101.00352">characterizing fairness over the set of good models under selective labels</a> accepted at ICML </li>
  <li><i>May 4, 2021</i> Invited talk at Johns Hopkins Causal Inference Working Group on counterfactual predictions for decision-making <a href="https://www.youtube.com/watch?v=Ubt_sH2qRMg">[Video]</a></li>
<li><i>April 22, 2021</i> Invited talk at PlaceKey COVID-19 Research Consortium on auditing mobility data for disparate coverage by race and age <a href="https://www.placekey.io/seminars/mobility-data-used-to-respond-to-covid19-could-be-biased">[Video]</a></li>
<li><i>April 16, 2021</i> CMU  <a href ="https://blog.ml.cmu.edu/2021/04/16/counterfactual-predictions-under-runtime-confounding/"> ML blog post </a> on counterfactual predictions under runtime confounding</li>
  <li><i>April 5, 2021</i> <i>Wall Street Journal</i> piece featured her research on auditing mobility data for demographic bias <a href ="https://www.wsj.com/articles/smartphone-location-data-can-leave-out-those-most-hit-by-covid-19-11617627615"> "Smartphone Location Data Can Leave Out Those Most Hit by Covid-19" </a></li>
  <li><i>November 18, 2020</i> <i>VentureBeat</i> piece featured her research on auditing mobility data for demographic bias <a href ="https://venturebeat.com/2020/11/18/stanford-and-carnegie-mellon-find-race-and-age-bias-in-mobility-data-that-drives-covid-19-policy/"> “Stanford and Carnegie Mellon find race and age bias in mobility data that drives COVID-19 policy” </a></li>
</ul>
      </div>
    </div>
  </div>


</div>
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.3.3/semantic.min.js"></script>
</body>
</html>