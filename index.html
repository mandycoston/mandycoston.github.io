<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }

  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }

  a.button, a.button:focus {
    font-size: 15px;
    font-weight: bold;
    border: 2px solid black;
    border-radius: 20px;
    margin: 5px;
    padding: 6px
  }
  a.button:hover {
  color: white;
  background-color: #1772d0;
  }
  
  body,td,th {
    font-family: 'Nunito', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  div.heading {
    font-family: 'Nunito', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600; /* 1000 */
  }
  div.authors {
    margin-top: 3px;
    margin-bottom: 8px
  }

  div.news {
    height: 250px;
    overflow-y: scroll
  }
  div.news::-webkit-scrollbar {
    -webkit-appearance: none;
    width: 7px;
  }
  div.news::-webkit-scrollbar-thumb {
    border-radius: 4px;
    background-color: rgba(0, 0, 0, .5);
    -webkit-box-shadow: 0 0 1px rgba(255, 255, 255, .5);
  }

  strong {
    font-family: 'Nunito', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Nunito', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Nunito', Verdana, Helvetica, sans-serif;
    font-size: 29px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Nunito', Verdana, Helvetica, sans-serif;
    font-size: 50px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }

  img.pub {
    padding: 0px;
    margin: 0px;
  }

  .paper {
    font-size: 12px;
    margin-left: 30px;
  }

  pre {
    border: solid;
    border-width: 1px;
    padding: 5px;
  }

  sticker {
    color:white;
    background-color: red;
    font-size: 13px;
    font-weight: bold;
    vertical-align: center;
    padding-left: 2px;
    padding-right: 2px;
  }

  </style>
  <link rel="shortcut icon" href="https://statistics.berkeley.edu/sites/all/themes/stats/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Amanda Coston</title>
  <meta name="Amanda Coston's Homepage" http-equiv="Content-Type" content="Amanda Coston's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Nunito:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <!-- <p align="left">asdf</p> -->
  <div style="width: 100%; display: inline-table; margin-bottom: 10px;">
  <div style="width: 20%; display: inline-table; text-align: center; vertical-align: middle;">
    <img src="ucb/seal.svg" height="102px">
  </div>
  <div style="width: 60%; display: inline-table; text-align: center;">
    <!-- <font size="7">Amanda Coston</font><br> -->
    <pageheading>Amanda Coston</pageheading><br>
    <b>email</b>:&nbsp
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'caoston@berkelee.ydu',
        [2,1, 3,4,5,6,7,8,9,10,11,12,13,14,15,18,17,16,19,20]);
    </script>
  </div>
  <div style="width: 20%; display: inline-table; text-align: center; vertical-align: middle;">
    <img src="ucb/stats-logo.png" height="90px">
  </div>
  </div>
  
  <tr>
    <td width="40%" valign="top"><a href="media/mandy_2025.jpg"><img src="media/mandy_2025.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    <a href="cv.pdf">CV</a> |
    <!-- <a href="bio.text" target="_blank">Bio</a> | -->
    <a href="https://scholar.google.com/citations?user=8U7d-_MAAAAJ&hl=en">Google Scholar</a> |
    <a href="https://github.com/mandycoston">Github</a> |
    <a href="https://twitter.com/amandacoston">Twitter</a>
    </p>
    </td>
    <td width="60%" valign="top" align="justify">
      <p>
        <a href= "https://statistics.berkeley.edu/people/amanda-coston">Amanda Coston</a> is an Assistant Professor in the <a href="https://statistics.berkeley.edu/">Department of Statistics</a> at <a href="https://www.berkeley.edu/">UC Berkeley</a>. Her work considers how -- and when -- machine learning and causal inference can improve decision-making in societally high-stakes settings. 
      </p>
      <p>
        Her research addresses real-world data problems that challenge the validity, equity, and reliability of algorithmic decision support systems and data-driven policy-making. A central focus of her research is identifying when algorithms, data used for policy-making, and human decisions disproportionately impact marginalized groups. 
      </p>
      <p>
        Amanda earned her PhD  in <a href="https://www.ml.cmu.edu/current-students/joint-phd-in-machine-learning-and-public-policy-requirements.html">Machine Learning and Public Policy</a> at <a href="https://www.cmu.edu">Carnegie Mellon University</a> (CMU) where she was advised by the incredible duo <a href= "https://www.andrew.cmu.edu/user/achoulde/">Alexandra Chouldechova</a> and <a href="http://www.ehkennedy.com/">Edward H. Kennedy.</a> Amanda completed a postdoc at <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a> in the <a href="https://www.microsoft.com/en-us/research/theme/machine-learning-statistics/">Machine Learning and Statistics Team</a>.
        Amanda is an <a href="https://statistics.berkeley.edu/about/news/coston-wins-okawa-grant">Okawa Foundation</a> grant recipient,
        a Schmidt Sciences <a href="https://www.schmidtsciences.org/ai2050-early-career-fellows-2024/">AI 2050 Early Career Fellow</a>, a Rising Star in <a href="https://risingstars.utexas.edu/">EECS</a>, <a href="https://ml.umd.edu/rising-stars">Machine Learning</a> and <a href="https://datascience.uchicago.edu/rising-stars/">Data Science</a>, <a href="https://research.facebook.com/fellows/coston-amanda-lee/"> Meta Research PhD Fellow</a>, <a href="https://www.nsfgrfp.org/resources/about-grfp/">NSF GRFP Fellow</a>, <a href="https://www.cmu.edu/news/stories/archives/2020/august/gates-presidential-fellows.html">K & L Gates Presidential Fellow</a> in Ethics and Computational Technologies, and a TCS Presidential Fellow. 
        <!--Her research on counterfactual risk assessments and evaluation for child welfare screening won the 2018 <a href="https://www.cmu.edu/cmnews/extra/060113_konda.html">Suresh Konda Best First Student Research Paper Award</a> from the <a href="https://www.heinz.cmu.edu/">Heinz College</a>.-->
        Her work has been recognized by best paper awards and featured in <a href ="https://www.wsj.com/articles/smartphone-location-data-can-leave-out-those-most-hit-by-covid-19-11617627615"><i>The Wall Street Journal</i></a> and <a href ="https://venturebeat.com/2020/11/18/stanford-and-carnegie-mellon-find-race-and-age-bias-in-mobility-data-that-drives-covid-19-policy/"> <i>VentureBeat</i></a>.
    </p>

    </td>
  </tr>
</table><hr>


<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;News</sectionheading></td></tr>
</table> -->

<!-- <div class="news">
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
  <tr>
    <td width="15%" valign="top" align="right"><b><i>March 4, 2024</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">   &#127942; Named an <a href="https://www.schmidtsciences.org/ai2050-early-career-fellows-2024/">AI 2050 Early Career Fellow</a>  by <a href="https://www.schmidtsciences.org/">Schmidt Sciences</a>
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Oct 30, 2023</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> &#127942; Honorable mention in <a href = "https://www.scs.cmu.edu/~scsfacts/dissertation.html"> School of Computer Science Distinguished Dissertation Award</a> for my thesis on <a href= "https://www.ml.cmu.edu/research/thesis_coston_amanda.pdf"> Principled Machine Learning for Societally Consequential Decision Making</a>.
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>July 10, 2023</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">  Started my postdoc at <a href = "https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/">Microsoft Research New England</a> on the <a href="https://www.microsoft.com/en-us/research/theme/machine-learning-statistics/">Machine Learning and Statistics Team</a>. 
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>June 12, 2023</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">   &#127942; <a href="https://facctconference.org/2023/acceptedpapers.html">Best Paper Award</a> at <a href="https://facctconference.org/">FAccT</a>  with Luke, Ken, and Zhiwei Steven for research on <a href= "https://dl.acm.org/doi/10.1145/3593013.3594101">counterfactual prediction</a> under measurement error and selection bias.
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>May 13, 2023</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">   &#127942; William W. Cooper Doctoral Dissertation Award in Management or Management Science from Carnegie Mellon University awarded for the dissertation on <a href= "https://www.ml.cmu.edu/research/thesis_coston_amanda.pdf"> Principled Machine Learning for
      Societally Consequential
      Decision Making</a>.
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Apr 27, 2023</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">  Defended my thesis on <a href= "https://www.ml.cmu.edu/research/thesis_coston_amanda.pdf"> Principled Machine Learning for
      Societally Consequential
      Decision Making</a> to my committee <a href = "https://www.andrew.cmu.edu/user/achoulde/"> Alex Chouldechova</a>, <a href = "https://www.ehkennedy.com/"> Edward Kennedy</a>, <a href = "https://www.cs.cmu.edu/~hheidari/">Hoda Heidari</a> and <a href = "https://sendhil.org/">Sendhil Mullainathan</a>.
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Apr 24, 2023</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">  Presented at <a href="https://simons.berkeley.edu/homepage">Simons Institute for the Theory of Computing</a> Workshop on <a href="https://simons.berkeley.edu/workshops/multigroup-fairness-validity-statistical-judgment">Multigroup Fairness and the Validity of Statistical Judgment</a>.
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Feb 9, 2023</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">   &#127942; <a href="https://satml.org/accepted-papers/">Best Paper Award</a> at <a href="https://satml.org/">SaTML</a> for our work on <a href = "https://arxiv.org/abs/2206.14983">evaluating justifiability of ML in high-stakes decisions</a> with Anna, Haiyi, Ken and Hoda.
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Nov 10, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> <sticker>TALK</sticker> Presented at the Symposium on Frontiers of Machine Learning & AI at <a href="https://viterbischool.usc.edu/">USC Viterbi</a>.
    </td>
  </tr>

  <tr>
    <td width="15%" valign="top" align="right"><b><i>Nov 01, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> <sticker>TALK</sticker> Presented at the ML seminar series at University of Maryland as a <a href="https://ml.umd.edu/rising-stars">Rising Star in Machine Learning</a>.
    </td>
  </tr>
  
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Oct 18, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> <sticker>TALK</sticker> Presented work on algorithmic fairness in the Rashomon set at  <a href="https://meetings.informs.org/wordpress/indianapolis2022/"> INFORMS</a> session on Finding Sets of Near-Optimal Solutions for MIPs. 
          </td>
  </tr>

  <tr>
    <td width="15%" valign="top" align="right"><b><i>Oct 13, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">
      &#127942; Accepted to <a href="https://datascience.uchicago.edu/rising-stars/">Rising Stars in Data Science</a> at the University of Chicago in Nov 2022!
    </td>
  </tr>

  <tr>
    <td width="15%" valign="top" align="right"><b><i>Oct 10, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">
      &#127942; Accepted to <a href="https://ml.umd.edu/rising-stars">Rising Stars in Machine Learning</a> at the University of Maryland in Nov 2022!
    </td>
  </tr>
 
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Oct 06, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Presented a poster on validity in decision making algorithms at <a href="https://eaamo.org/#home"> EAAMO</a> and attending the doctoral consortium!  
          </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Oct 02, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> <sticker>TALK</sticker> Invited talk on counterfactual audit for racial bias at <a href="http://www.ams.org/meetings/sectional/2301_progfull.html"> American Mathematical Society Sectional Meeting on Causality</a>. 
          </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Sep 23, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> <sticker>TALK</sticker> Presented work on counterfactual risk assessments under unmeasured confounding at  <a href="https://economics.brown.edu/orlando-bravo-center-economic-research"> Brown's Bravo Center Workshop on the Economics of Algorithms</a>. 
    </td>
  </tr> 
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Sep 12, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Check out our <a href="counterfactual-under-unmeasured-conf-RCK.pdf"> draft</a> of counterfactual risk assessments under unmeasured confounding with Ashesh Rambachan and Ed Kennedy.
  </td>
  </tr> 
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Aug 16, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Attended the  <a href="https://cra.org/ccc/events/artificial-intelligence-operations-research-workshop-ii/"> CCC and INFORMS Artificial Intelligence/Operations Research Workshop </a>in Atlanta, GA.
          </td>
  </tr>

  <tr>
    <td width="15%" valign="top" align="right"><b><i>Aug 01, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">
      &#127942; Accepted to <a href="https://risingstars.utexas.edu/">Rising Stars in EECS</a> at UT-Austin in Oct 2022!
    </td>
  </tr>

<tr>
    <td width="15%" valign="top" align="right"><b><i>Jul 19, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">  <a href="https://arxiv.org/abs/2206.14983"> Validity in ML paper</a> accepted to <a href="https://eaamo.org/#home">Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO)</a> 
      (joint work with <a href="https://annakawakami.com/">Anna</a>, <a href="https://haiyizhu.com/">Haiyi</a>, <a href="https://www.thecoalalab.com/kenholstein">Ken</a>, and <a href="https://www.cs.cmu.edu/~hheidari/">Hoda</a>). 
         </td> 
  </tr>
  
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Jul 19, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Released preprint of our manuscript on <a href="https://arxiv.org/abs/2207.09016"> role of the geometric mean in case-control studies</a> (joint work with <a href="http://www.ehkennedy.com/">Edward Kennedy</a>). 
          </td>
  </tr>

  <tr>
    <td width="15%" valign="top" align="right"><b><i>Jul 15, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> <sticker>TALK</sticker> Presented work on auditing administrative data for bias at <a href="https://reglab.stanford.edu/"> Stanford RegLab</a>. 
          </td>
  </tr>
  
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Jun 30, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Released a pre-print of <a href="https://arxiv.org/abs/2206.14983"> our paper</a>  on using validity as a lens to evaluate justified use of data-driven decision making  
       (joint work with <a href="https://annakawakami.com/">Anna</a>, <a href="https://haiyizhu.com/">Haiyi</a>, <a href="https://www.thecoalalab.com/kenholstein">Ken</a>, and <a href="https://www.cs.cmu.edu/~hheidari/">Hoda</a>). 
          </td>
  </tr>
  
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Jun 24, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Chaired the session on <a href ="https://facctconference.org/2022/schedule.html">responsible data management</a> at ACM FAccT 2022. 
          </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>May 24, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> <sticker>TALK</sticker> Oral presentation at <a href="https://ctml.berkeley.edu/acic-2022-room-1"> ACIC</a> on counterfactual audits for racial bias in police traffic stops (joint work with <a href="http://www.ehkennedy.com/">Edward Kennedy</a>). 
          </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>May 24, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Poster session at ACIC on counterfactual risk assessments under unmeasured confounding (joint work with <a href= "https://asheshrambachan.github.io/">Ashesh Rambachan</a> and <a href="http://www.ehkennedy.com/">Edward Kennedy</a>). 
          </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Mar 15, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">
      &#127942; Accepted to <a href="https://facctconference.org/2022/cfdc.html">FAccT doctoral consortium</a> in Seoul, South Korea!
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Feb 12, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">
      &#127942; Accepted to University of Michigan <a href="https://midas.umich.edu/future-leaders-summit-2022/">Michigan Future Leaders Summit</a> hosted by <a href="https://midas.umich.edu/">Michigan Institute for Data Science (MIDAS)</a>!
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Feb 4, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> Proposed thesis, <a href="https://www.cs.cmu.edu/calendar/157530883">Principled Machine Learning for High-stakes Decisions</a>. <br> Committee: <a href="http://www.ehkennedy.com/">Ed Kennedy</a>, <a href= "https://www.andrew.cmu.edu/user/achoulde/">Alex Chouldechova</a>, <a href="https://www.cs.cmu.edu/~hheidari/">Hoda Heidari</a>, & <a href="https://sendhil.org/">Sendhil Mullainathan</a> 
    </td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Feb 2, 2022</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"> &#127942; Awarded the <a href="https://research.facebook.com/fellows/coston-amanda-lee/"> Meta Research PhD Fellowship</a>! Thanks Meta Research for the support!</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Oct 21, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"><sticker>TALK</sticker> Invited talk at <a href="https://www.merck.com/">Merck</a> Data Science All Hands</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Sep, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">Joined the <a href="https://www.cmu.edu/stugov/gsa/About-the-GSA/Committees.html">Graduate Student Assembly Campus Affairs Committee</a> where I will focus on sustainability efforts at CMU.</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Jun 07, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">Started internship at <a href="https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/">Facebook Responsible AI</a>.</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>May 18, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">Featured on <a href="https://www.placekey.io/blog/amanda-coston-carnegie-mellon">Placekey Spotlight</a>.</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>May 08, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">Our <a href="http://proceedings.mlr.press/v139/coston21a.html">research paper</a> on characterizing fairness over the set of good models under selective labels accepted at <a href="https://icml.cc/Conferences/2021">ICML 2021</a>.</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>May 04, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"><sticker>TALK</sticker> Invited talk at <a href="https://jhsphcausalinference.weebly.com/">Johns Hopkins Causal Inference Working Group</a> on counterfactual predictions for decision-making. Check out the <a href="https://www.youtube.com/watch?v=Ubt_sH2qRMg">video here</a>!</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Apr 22, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"><sticker>TALK</sticker> Invited talk at <a href="https://www.placekey.io/">PlaceKey</a> COVID-19 Research Consortium on auditing mobility data for disparate coverage by race and age. Check out the <a href="https://www.placekey.io/seminars/mobility-data-used-to-respond-to-covid19-could-be-biased">video here</a>!</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Apr 16, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;">CMU <a href ="https://blog.ml.cmu.edu/2021/04/16/counterfactual-predictions-under-runtime-confounding/"> ML Blog Post </a> on counterfactual predictions under runtime confounding.</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Apr 05, 2021</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"><i>The Wall Street Journal</i> featured our research on auditing mobility data for demographic bias! The piece is titled <a href ="https://www.wsj.com/articles/smartphone-location-data-can-leave-out-those-most-hit-by-covid-19-11617627615"><i>Smartphone Location Data Can Leave Out Those Most Hit by Covid-19</i></a>.</td>
  </tr>
  <tr>
    <td width="15%" valign="top" align="right"><b><i>Nov 18, 2020</i></b></td>
    <td width="85%" valign="top" align="left" style="padding-left: 10px;"><i>VentureBeat</i> featured our research on auditing mobility data for demographic bias! The piece is titled <a href ="https://venturebeat.com/2020/11/18/stanford-and-carnegie-mellon-find-race-and-age-bias-in-mobility-data-that-drives-covid-19-policy/"> <i>Stanford and Carnegie Mellon find race and age bias in mobility data that drives COVID-19 policy</i></a>.</td>
  </tr>
</table>
</div>
<hr style="margin-top: 20px;"> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>
        &nbsp;&nbsp;Research talk
        <span style="float: right; font-weight:normal; font-size: 26px;">(Simons Institute, 1/15/26)</span>
      </sectionheading>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
  <center>
  <iframe width="95%" height="325px" src="https://www.youtube.com/embed/WUiTKvhjn90?start=68" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
</table>
<hr style="margin-top: 20px;">

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Awards</sectionheading></td></tr>
</table>

<div style="height: 200px; overflow-y: scroll;">
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2025</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;">
      Awarded the <a href="https://statistics.berkeley.edu/about/news/coston-wins-okawa-grant">Okawa Foundation</a> research grant.
    </td>
  </tr>
  <tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2024</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;">
      <a href="https://www.schmidtsciences.org/ai2050-early-career-fellows-2024/">AI 2050 Early Career Fellowship</a>  by <a href="https://www.schmidtsciences.org/">Schmidt Sciences</a>.
    </td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2023</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;">
      Honorable mention in <a href = "https://www.scs.cmu.edu/~scsfacts/dissertation.html">CMU School of Computer Science Distinguished Dissertation Award</a>.
      <!-- on <a href= "https://www.ml.cmu.edu/research/thesis_coston_amanda.pdf"> Principled Machine Learning for Societally Consequential Decision Making</a>. -->
    </td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2023</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"> William W. Cooper Doctoral Dissertation Award in Management or Management Science from Carnegie Mellon University for the dissertation <a href= "https://www.ml.cmu.edu/research/thesis_coston_amanda.pdf"> Principled Machine Learning for
      Societally Consequential
      Decision Making</a>.</td>
  </tr>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2023</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://facctconference.org/2023/acceptedpapers.html">Best Paper Award</a> at <a href="https://facctconference.org/">FAccT</a>  with Luke, Ken, and Zhiwei Steven for <a href= "https://dl.acm.org/doi/10.1145/3593013.3594101">counterfactual prediction</a> under measurement error and selection bias.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2023</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"> William W. Cooper Doctoral Dissertation Award in Management or Management Science from Carnegie Mellon University for the dissertation <a href= "https://www.ml.cmu.edu/research/thesis_coston_amanda.pdf"> Principled Machine Learning for
      Societally Consequential
      Decision Making</a>.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2023</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://satml.org/accepted-papers/">Best Paper Award</a> at <a href="https://satml.org/">SaTML</a>  with Anna, Haiyi, Ken and Hoda for <a href= "https://arxiv.org/abs/2206.14983">centering validity</a> in evaluating justified use of algorithms.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2022</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://datascience.uchicago.edu/rising-stars/">Rising Star in Data Science</a> at the University of Chicago.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2022</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://ml.umd.edu/rising-stars">Rising Star in Machine Learning</a> at the University of Maryland.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2022</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://risingstars.utexas.edu/">Rising Star in EECS 2022</a> at UT-Austin.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2022</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://research.facebook.com/fellows/coston-amanda-lee/"> Meta Research PhD Fellow</a>.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2022</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://midas.umich.edu/future-leaders-summit-2022/">Future Leader in Responsible Data Science</a>.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2020</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://www.cmu.edu/news/stories/archives/2020/august/gates-presidential-fellows.html">K & L Gates Presidential Fellow</a> in Ethics and Computational Technologies.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2019</i></b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;">Tata Consultancy Services (TCS) Presidential Fellowship.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2018</i></b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://www.cmu.edu/cmnews/extra/060113_konda.html">Suresh Konda Best First Student Research Paper Award</a> from the <a href="https://www.heinz.cmu.edu/">Heinz College</a> for <a href="https://arxiv.org/abs/1909.00066" target="_blank">counterfactual evaluation in child welfare</a>.</td>
  </tr>
  <tr>
    <td width="12%" valign="top" align="right"><b><i>2018</i></b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;"><a href="https://www.nsfgrfp.org/resources/about-grfp/">NSF GRFP Fellow</a>.</td>
  </tr>
</table>
</div>
<hr style="margin-top: 20px;">

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Working papers</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/abs/2601.17146" target="_blank">
        <img class="pub" src="website-photos/falsifying.png" alt="Falsifying Predictive Algorithms: Gender and Race rank distributions" width="80%" style="border: 1px solid #000;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2601.17146" target="_blank">
      <div class="heading">Falsifying Predictive Algorithms</div></a>
      <div class="authors"><b>A. Coston</b></div>
      <i>ArXiv Preprint</i>, 2026
      </p>
      <div>
      <a href="javascript:toggleblock('falsifying_abs')">Abstract</a> |
      <a href="https://arxiv.org/abs/2601.17146" target="_blank">ArXiv</a>
      <p align="justify"> <i id="falsifying_abs">Empirical investigations into unintended model behavior often show that the algorithm is predicting another outcome than what was intended. These exposes highlight the need to identify when algorithms predict unintended quantities &mdash; ideally before deploying them into consequential settings. We propose a falsification framework that provides a principled statistical test for discriminant validity: the requirement that an algorithm predict intended outcomes better than impermissible ones. Drawing on falsification practices from causal inference, econometrics, and psychometrics, our framework compares calibrated prediction losses across outcomes to assess whether the algorithm exhibits discriminant validity with respect to a specified impermissible proxy. In settings where the target outcome is difficult to observe, multiple permissible proxy outcomes may be available; our framework accommodates both this setting and the case with a single permissible proxy. Throughout we use nonparametric hypothesis testing methods that make minimal assumptions on the data-generating process. We illustrate the method in an admissions setting, where the framework establishes discriminant validity with respect to gender but fails to establish discriminant validity with respect to race. This demonstrates how falsification can serve as an early validity check, prior to fairness or robustness analyses. We also provide analysis in a criminal justice setting, where we highlight the limitations of our framework and emphasize the need for complementary approaches to assess other aspects of construct validity and external validity.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('falsifying_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/abs/2507.05216" target="_blank">
        <img class="pub" src="website-photos/bridging.png" alt="Science of Algorithmic Decision Support: Prediction as Intervention diagram" width="80%" style="border: 1px solid #000;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2507.05216" target="_blank">
      <div class="heading">Bridging Prediction and Intervention Problems in Social Systems</div></a>
      <div class="authors">L. T. Liu, I. D. Raji, A. Zhou, L. Guerdan, J. Hullman, D. Malinsky, B. Wilder, S. Zhang, H. Adam, <b>A. Coston</b>, B. Laufer, et al.</div>
      <i>ArXiv Preprint</i>, 2025
      </p>
      <div>
      <a href="javascript:toggleblock('bridging_abs')">Abstract</a> |
      <a href="https://arxiv.org/abs/2507.05216" target="_blank">ArXiv</a>
      <p align="justify"> <i id="bridging_abs">Many automated decision systems (ADS) are designed to solve prediction problems &mdash; where the goal is to learn patterns from a sample of the population and apply them to individuals from the same population. In reality, these prediction systems operationalize holistic policy interventions in deployment. Once deployed, ADS can shape impacted population outcomes through an effective policy change in how decision-makers operate, while also being defined by past and present interactions between stakeholders and the limitations of existing organizational, as well as societal, infrastructure and context. In this work, we consider the ways in which we must shift from a prediction-focused paradigm to an intervention-oriented paradigm when considering the impact of ADS within social systems. We argue this requires a new default problem setup for ADS beyond prediction, to instead consider predictions as decision support, final decisions, and outcomes. We highlight how this perspective unifies modern statistical frameworks and other tools to study the design, implementation, and evaluation of ADS systems, and point to the research directions necessary to operationalize this paradigm shift. Using these tools, we characterize the limitations of focusing on isolated prediction tasks, and lay the foundation for a more intervention-oriented approach to developing and deploying ADS.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('bridging_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://www.youtube.com/watch?v=UuDdxtQEc7I&list=PLHtzc5hGcx9sjwuETR3izb27TJSauQ4EO&autoplay=0" target="_blank"><img class="pub" src="website-photos/madison-police.png" alt="image not found" width="80%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://www.youtube.com/watch?v=UuDdxtQEc7I&list=PLHtzc5hGcx9sjwuETR3izb27TJSauQ4EO7&autoplay=0" target="_blank">
      <div class="heading">Counterfactual audit of racial bias in police traffic stops</div></a>
      <div class="authors"><b>A. Coston</b>, E. H. Kennedy</div>
      <i>American Causal Inference Conference (ACIC) </i>, 2022</i>
      <div style="color: red;"><i>Oral presentation (20% selection rate)</i></div>
      </p>

      <div>
      <a href="javascript:toggleblock('police_stops_abs')">Abstract</a> |
      <a href="https://www.youtube.com/watch?v=UuDdxtQEc7I&list=PLHtzc5hGcx9sjwuETR3izb27TJSauQ4EO&autoplay=0" target="_blank">Talk</a>


      <p align="justify"> <i id="police_stops_abs">Racial bias in criminal justice has profound consequences. This bias harms not only the participant of the encounter but also has far-reaching implications, particularly in the era of big data. These interactions are often recorded and used to build algorithms that shape future encounters with the criminal justice system. In order to guard against automating biases in these high-stakes algorithms, we must identify when and where bias occurs in the criminal justice pipeline. 

        We consider racial bias in one of the most common points of entry: police traffic stops. Building on the tradition that uses the "veil of darkness" to test for racial bias in officer's decisions to stop, we propose a counterfactual audit for racial bias that clarifies the assumptions needed to identify racial bias. A central challenge is that standard measures of effect are not identifiable due to the outcome-dependent sampling of police traffic stop data. As a solution, we identify an odds ratio that recovers a test for differences in the race-conditional risk ratios under Bernoulli sampling. We propose an efficient estimator for the identified odds ratio using doubly-robust techniques that allow for flexible, non-parametric estimation. We present empirical results on the Stanford Open Policing data.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('police_stops_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr><td><div></div></td></tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2207.09016" target="_blank"><img class="pub" src="website-photos/geomean.png" alt="image not found" width="80%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2207.09016" target="_blank">
      <div class="heading">The role of the geometric mean in case-control studies</div></a>
      <div class="authors"><b>A. Coston</b>, E. H. Kennedy</div>
      <i>ArXiv Preprint</i>, 2022</i>
      </p>

      <div>
      <a href="javascript:toggleblock('geometric_mean_abs')">Abstract</a> |
      <a href="https://arxiv.org/abs/2207.09016" target="_blank">ArXiv</a>

      <p align="justify"> <i id="geometric_mean_abs">Historically used in settings where the outcome is rare or data collection is expensive, outcome-dependent sampling is relevant to many modern settings where data is readily available for a biased sample of the target population, such as public administrative data. Under outcome-dependent sampling, common effect measures such as the average risk difference and the average risk ratio are not identified, but the conditional odds ratio is. Aggregation of the conditional odds ratio is challenging since summary measures are generally not identified. Furthermore, the marginal odds ratio can be larger (or smaller) than all conditional odds ratios. This so-called non-collapsibility of the odds ratio is avoidable if we use an alternative aggregation to the standard arithmetic mean. We provide a new definition of collapsibility that makes this choice of aggregation method explicit, and we demonstrate that the odds ratio is collapsible under geometric aggregation. We describe how to partially identify, estimate, and do inference on the geometric odds ratio under outcome-dependent sampling. Our proposed estimator is based on the efficient influence function and therefore has doubly robust-style properties.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('geometric_mean_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2212.09844" target="_blank"><img class="pub" src="website-photos/unmeasured-confounding.png" alt="image not found" width="80%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2212.09844" target="_blank">
      <div class="heading">Robust design and evaluation of predictive algorithms
        under unmeasured confounding</div></a>
      <div class="authors">A. Rambachan, <b>A. Coston</b>, E. H. Kennedy</div>
      <i>&#8226; American Causal Inference Conference (ACIC)</i>, 2022<br>
      <i>&#8226; NeurIPS Workshop on Algorithmic Fairness through the Lens of Causality and Privacy</i>, 2022
      </p>

      <div>
      <a href="javascript:toggleblock('cfactual_risk_asses_abs')">Abstract</a> |
      <a href="https://arxiv.org/abs/2212.09844" target="_blank">ArXiv</a>

      <p align="justify"> <i id="cfactual_risk_asses_abs">Predictive algorithms inform consequential decisions in settings where the outcome is selectively observed given some choices made by human decision makers. There often exists unobserved confounders that affected the decision maker's choice and the outcome. We propose a unified methodology for the robust design and evaluation of predictive algorithms in selectively observed data under such unobserved confounding. Our approach imposes general assumptions on how much the outcome may vary on average between unselected and selected units conditional on observed covariates and identified nuisance parameters, formalizing popular empirical strategies for imputing missing data such as proxy outcomes and instrumental variables. We develop debiased machine learning estimators for the bounds on a large class of predictive performance estimands, such as the conditional likelihood of the outcome, a predictive algorithm's mean square error, true/false positive rate, and many others, under these assumptions. In an administrative dataset from a large Australian financial institution, we illustrate how varying assumptions on unobserved confounding leads to meaningful changes in default risk predictions and evaluations of credit scores across sensitive groups.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('cfactual_risk_asses_abs');
      </script>
      </div>
    </td>
  </tr>

</table>
<br>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Selected Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2404.00848" target="_blank"><img class="pub" src="website-photos/pred_perf_comp.png" alt="image not found" width="75%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2404.00848" target="_blank">
      <div class="heading">Predictive Performance Comparison of Decision
        Policies Under Confounding.</div></a>
      <div class="authors">L. Guerdan, <b>A. Coston</b>, Z. S. Wu, K Holstein</div>
      <i>International Conference on Machine Learning (ICML)</i>, 2024
      </p>

      <div>
      <a href="javascript:toggleblock('pred_perf_comp_abs')">Abstract</a> |
      <a href="https://proceedings.mlr.press/v235/guerdan24a.html" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2404.00848" target="_blank">ArXiv</a>
      <p align="justify"> <i id="pred_perf_comp_abs">Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('pred_perf_comp_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2302.11121" target="_blank"><img class="pub" src="website-photos/measurement.png" alt="image not found" width="80%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2302.11121" target="_blank">
      <div class="heading">Counterfactual Prediction Under Outcome Measurement Error</div></a>
      <div class="authors"> L. Guerdan, <b>A. Coston</b>, K. Holstein, Z.S. Wu</div>
      <i>ACM Conference on Fairness, Accountability, and Transparency (FAccT)</i>, 2023.
      <span style='color:red; margin-left: 5px;'>&#127942; Best Paper Award</span>
      </p>

      <div>
      <a href="javascript:toggleblock('facct23__measurement_abs')">Abstract</a> |
      <a href="https://dl.acm.org/doi/10.1145/3593013.3594101" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2302.11121" target="_blank">ArXiv</a> |
      <a href="https://www.youtube.com/watch?v=l5cXiJjQNWc" target="_blank">Talk</a>


      <p align="justify"> <i id="facct23__measurement_abs">Across domains such as medicine, employment, and criminal justice, predictive models often target labels that imperfectly reflect the outcomes of interest to experts and policymakers. For example, clinical risk assessments deployed to inform physician decision-making often predict measures of healthcare utilization (e.g., costs, hospitalization) as a proxy for patient medical need. These proxies can be subject to outcome measurement error when they systematically differ from the target outcome they are intended to measure. However, prior modeling efforts to characterize and mitigate outcome measurement error overlook the fact that the decision being informed by a model often serves as a risk-mitigating intervention that impacts the target outcome of interest and its recorded proxy. Thus, in these settings, addressing measurement error requires counterfactual modeling of treatment effects on outcomes. In this work, we study intersectional threats to model reliability introduced by outcome measurement error, treatment effects, and selection bias from historical decision-making policies. We develop an unbiased risk minimization method which, given knowledge of proxy measurement error properties, corrects for the combined effects of these challenges. We also develop a method for estimating treatment-dependent measurement error parameters when these are unknown in advance. We demonstrate the utility of our approach theoretically and via experiments on real-world data from randomized controlled trials conducted in healthcare and employment domains. As importantly, we demonstrate that models correcting for outcome measurement error or treatment effects alone suffer from considerable reliability limitations. Our work underscores the importance of considering intersectional threats to model validity during the design and evaluation of predictive models for decision support.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('facct23__measurement_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2302.06503" target="_blank"><img class="pub" src="website-photos/groundless.png" alt="image not found" width="80%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2302.06503" target="_blank">
      <div class="heading"> Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making</div></a>
      <div class="authors">L. Guerdan, <b>A. Coston</b>, Z. S. Wu, K. Holstein</div>
      <i>ACM Conference on Fairness, Accountability, and Transparency (FAccT)</i>, 2023
      </p>

      <div>
      <a href="javascript:toggleblock('groundless_abs')">Abstract</a> |
      <a href="https://dl.acm.org/doi/10.1145/3593013.3594036" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2302.06503" target="_blank">ArXiv</a> |
      <a href="https://www.youtube.com/watch?v=tlFrxRlXK4s" target="_blank">Talk</a>

      <p align="justify"> <i id="groundless_abs">A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on "ground truth" labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans reason about broader phenomena of interest in a decision - including latent constructs that are not directly observable, such as disease status, the "toxicity" of online comments, or future "job performance" - predictive models target proxy labels that are readily available in existing datasets. Predictive models' reliance on simplistic proxies makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.
      </i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('groundless_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://openreview.net/forum?id=LghfT9-phCc" target="_blank"><img class="pub" src="website-photos/justi.png" alt="image not found" width="80%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://openreview.net/forum?id=LghfT9-phCc" target="_blank">
      <div class="heading">A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms</div></a>
      <div class="authors"><b>A. Coston</b>, A. Kawakami, H. Zhu, K. Holstein, H. Heidari</div>
      <i>IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)</i>, 2023.
      <span style='color:red; margin-left: 5px;'>&#127942; Best Paper Award</span>
      </p>

      <div>
      <a href="javascript:toggleblock('satml23_abs')">Abstract</a> |
      <a href="https://openreview.net/forum?id=LghfT9-phCc" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2206.14983" target="_blank">ArXiv</a> 


<p align="justify"> <i id="satml23_abs">Recent research increasingly brings to question the appropriateness of using predictive tools in complex, real-world tasks. While a growing body of work has explored ways to improve value alignment in these tools, comparatively less work has centered concerns around the fundamental justifiability of using these tools. This work seeks to center validity considerations in deliberations around whether and how to build data-driven algorithms in high-stakes domains. Toward this end, we translate key concepts from validity theory to predictive algorithms. We apply the lens of validity to re-examine common challenges in problem formulation and data issues that jeopardize the justifiability of using predictive algorithms and connect these challenges to the social science discourse around validity. Our interdisciplinary exposition clarifies how these concepts apply to algorithmic decision making contexts. 
  We demonstrate how these validity considerations could distill into a series of high-level questions intended to promote and document reflections on the legitimacy of the predictive task and the suitability of the data.</i></p>
<script xml:space="preserve" language="JavaScript">
  hideblock('satml23_abs');
</script>
</div>
</td>
</tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2011.07194" target="_blank"><img class="pub" src="website-photos/leveraging.png" alt="image not found" width="80%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2011.07194" target="_blank">
      <div class="heading">Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy</div></a>
      <div class="authors"><b>A. Coston</b>, N. Guha, L. Lu, D. Ouyang, A. Chouldechova, D. Ho</div>
      <i>ACM Conference on Fairness, Accountability, and Transparency (FAccT)</i>, 2021
      </p>

      <div>
      <a href="javascript:toggleblock('facct21_abs')">Abstract</a> |
      <a href="https://dl.acm.org/doi/10.1145/3442188.3445881" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2011.07194" target="_blank">ArXiv</a> |
      <a href="https://www.youtube.com/watch?v=K2axD2sv-Uc" target="_blank">Talk</a>


      <p align="justify"> <i id="facct21_abs">Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data containing individual-level voter turnout for specific voting locations along with race and age can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('facct21_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2101.00352" target="_blank"><img class="pub" src="website-photos/characterizing-fairness.png" alt="image not found" width="75%" style="border-width: 1px; border-style: solid; border-color: black;"></a></td>
    <td width="55%" valign="top">
      <p><a href="https://arxiv.org/abs/2101.00352" target="_blank">
      <div class="heading">Characterizing Fairness over the Set of Good Models Under Selective Labels</div></a>
      <div class="authors"><b>A. Coston</b>, A. Rambachan, A. Chouldechova</div>
      <i>International Conference on Machine Learning (ICML)</i>, 2021
      </p>

      <div>
      <a href="javascript:toggleblock('icml21_abs')">Abstract</a> |
      <a href="http://proceedings.mlr.press/v139/coston21a.html" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2101.00352" target="_blank">ArXiv</a> |
      <a href="https://icml.cc/virtual/2021/poster/8471" target="_blank">Talk</a>


      <p align="justify"> <i id="icml21_abs">Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the "Rashomon Effect." These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or "the set of good models." Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) replace an existing model with one that has better fairness properties; or 2) audit for predictive bias. We illustrate these uses cases on a real-world credit-scoring task and a recidivism prediction task.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('icml21_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr><td><div></div></td></tr>
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2006.16916" target="_blank"><img class="pub" src="website-photos/runtime-confounding.png" alt="image not found" width="85%"></a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2006.16916" target="_blank">
      <div class="heading">Counterfactual Predictions under Runtime Confounding</div></a>
      <div class="authors"><b>A. Coston</b>, E. H. Kennedy, A. Chouldechova</div>
      <i>Neural Information Processing Systems (NeurIPS)</i>, 2020
      </p>

      <div>
      <a href="javascript:toggleblock('neurips20_abs')">Abstract</a> |
      <a href="https://proceedings.neurips.cc/paper/2020/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2006.16916" target="_blank">ArXiv</a> |
      <a href="https://blog.ml.cmu.edu/2021/04/16/counterfactual-predictions-under-runtime-confounding/" target="_blank">Blog</a><br><br>


      <p align="justify"> <i id="neurips20_abs">Algorithms are commonly used to predict outcomes under a particular decision or intervention, such as predicting whether an offender will succeed on parole if placed under minimal supervision. Generally, to learn such counterfactual prediction models from observational data on historical decisions and corresponding outcomes, one must measure all factors that jointly affect the outcomes and the decision taken. Motivated by decision support applications, we study the counterfactual prediction task in the setting where all relevant factors are captured in the historical data, but it is either undesirable or impermissible to use some such factors in the prediction model. We refer to this setting as runtime confounding. We propose a doubly-robust procedure for learning counterfactual prediction models in this setting. Our theoretical analysis and experimental results suggest that our method often outperforms competing approaches. We also present a validation procedure for evaluating the performance of counterfactual prediction methods.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('neurips20_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="left"><a href="https://arxiv.org/abs/1909.00066" target="_blank"><img class="pub" src="website-photos/counterfactual-evaluation.png" alt="image not found" width="90%"></a></td>
    <td width="55%" valign="top">
      <p><a href="https://arxiv.org/abs/1909.00066" target="_blank">
      <div class="heading">Counterfactual Risk Assessments, Evaluation, and Fairness</div></a>
      <div class="authors"><b>A. Coston</b>, A. Mishler, E. H. Kennedy, A. Chouldechova</div>
      <i>ACM Conference on Fairness, Accountability, and Transparency (FAccT)</i>, 2020
      <span style='color:red; margin-left: 5px;'>&#127942; Suresh Konda Best First Paper Award</span>
      </p>

      <div>
      <a href="javascript:toggleblock('facct20_abs')">Abstract</a> |
      <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372851" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/1909.00066" target="_blank">ArXiv</a> |
      <a href="https://www.youtube.com/watch?v=9zfi3heBYUs" target="_blank">Talk</a>


      <p align="justify"> <i id="facct20_abs">Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may --- and as we show empirically, do --- induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('facct20_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="left"><a href="https://arxiv.org/abs/1910.07162" target="_blank"><img class="pub" src="website-photos/conditional-learning-fair-representations.png" alt="image not found" width="90%"></a></td>
    <td width="65%" valign="top">
      <p><a href="https://arxiv.org/abs/1910.07162" target="_blank">
      <div class="heading">Conditional Learning of Fair Representations</div></a>
      <div class="authors">H. Zhao, <b>A. Coston</b>, T. Adel, G. J. Gordon</div>
      <i>International Conference on Learning Representations <br>(ICLR)</i>, 2020
      </p>

      <div>
      <a href="javascript:toggleblock('iclr20_abs')">Abstract</a> |
      <a href="https://openreview.net/forum?id=Hkekl0NFPr" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/1910.07162" target="_blank">ArXiv</a> |
      <a href="https://iclr.cc/virtual_2020/poster_Hkekl0NFPr.html" target="_blank">Talk</a>


      <p align="justify"> <i id="iclr20_abs">We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups in the classification setting. Two key components underpinning the design of our algorithm are balanced error rate and conditional alignment of representations. We show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity. Furthermore, we also demonstrate both in theory and on two real-world experiments that the proposed algorithm leads to a better utility-fairness trade-off on balanced datasets compared with existing algorithms on learning fair representations for classification.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('iclr20_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2007.07796" target="_blank"><img class="pub" src="website-photos/neural-topic-models-survival-small.png" alt="image not found" width="90%"></a></td>
    <td width="65%" valign="top">
      <p><a href="https://arxiv.org/abs/2007.07796" target="_blank">
      <div class="heading">Neural Topic Models with Survival Supervision: Jointly Predicting Time-to-Event Outcomes and Learning How Clinical Features Relate</div></a>
      <div class="authors">L. Li, R. Zuo, <b>A. Coston</b>, J. C. Weiss, G. H. Chen</div>
      <i>International Conference on Artificial Intelligence in Medicine</i>, 2020
      </p>

      <div>
      <a href="javascript:toggleblock('icaim20_abs')">Abstract</a> |
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-59137-3_33" target="_blank">Paper</a> |
      <a href="https://arxiv.org/abs/2007.07796" target="_blank">ArXiv</a>


      <p align="justify"> <i id="icaim20_abs">In time-to-event prediction problems, a standard approach to estimating an interpretable model is to use Cox proportional hazards, where features are selected based on lasso regularization or stepwise regression. However, these Cox-based models do not learn how different features relate. As an alternative, we present an interpretable neural network approach to jointly learn a survival model to predict time-to-event outcomes while simultaneously learning how features relate in terms of a topic model. In particular, we model each subject as a distribution over "topics", which are learned from clinical features as to help predict a time-to-event outcome. From a technical standpoint, we extend existing neural topic modeling approaches to also minimize a survival analysis loss function. We study the effectiveness of this approach on seven healthcare datasets on predicting time until death as well as hospital ICU length of stay, where we find that neural survival-supervised topic models achieves competitive accuracy with existing approaches while yielding interpretable clinical "topics" that explain feature relationships.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('icaim20_abs');
      </script>
      </div>
    </td>
  </tr>

  <tr><td><div></div></td></tr>
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://dl.acm.org/doi/10.1145/3306618.3314236" target="_blank"><img class="pub" src="website-photos/transfer-learning.png" alt="image not found" width="85%"></a></td>
    <td width="65%" valign="top">
      <p><a href="https://dl.acm.org/doi/10.1145/3306618.3314236" target="_blank">
      <div class="heading">Fair Transfer Learning with Missing Protected Attributes</div></a>
      <div class="authors"><b>A. Coston</b>, K. N. Ramamurthy, D. Wei, K. R. Varshney, S. Speakman, Z. Mustahsan, S. Chakraborty</div>
      <i>AAAI/ACM Conference on Artificial Intellligence, Ethics, and Society (AIES)</i>, 2019
      </p>

      <div>
      <a href="javascript:toggleblock('aies19_abs')">Abstract</a> |
      <a href="https://dl.acm.org/doi/10.1145/3306618.3314236" target="_blank">Paper</a>


      <p align="justify"> <i id="aies19_abs">Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.</i></p>
      <script xml:space="preserve" language="JavaScript">
        hideblock('aies19_abs');
      </script>
      </div>
    </td>
  </tr>


  <!-- <hr style="margin-top: 20px;">
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tr><td><sectionheading>&nbsp;&nbsp;Working Papers</sectionheading></td></tr>
  </table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  
    <tr>
      <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2011.07194" target="_blank"><img class="pub" src="website-photos/geomean.png" alt="image not found" width="80%"></a></td>
      <td width="60%" valign="top">
        <p><a href="https://arxiv.org/abs/2011.07194" target="_blank">
        <div class="heading">The Role of the Geometric Mean in Case-Control Studies
        </div></a>
        <div class="authors"><b>A. Coston</b>, E. H. Kennedy.</div>
        </p>
  
        <div>
        <a href="javascript:toggleblock('facct21_abs')">Abstract</a> |
        <a href="https://arxiv.org/abs/2207.09016" target="_blank">ArXiv</a> 
  
  
        <p align="justify"> <i id="geomean2022_abs">Historically used in settings where the outcome is rare or data collection is expensive, outcome-dependent sampling is relevant to many modern settings where data is readily available for a biased sample of the target population, such as public administrative data. Under outcome-dependent sampling, common effect measures such as the average risk difference and the average risk ratio are not identified, but the conditional odds ratio is. Aggregation of the conditional odds ratio is challenging since summary measures are generally not identified. Furthermore, the marginal odds ratio can be larger (or smaller) than all conditional odds ratios. This so-called non-collapsibility of the odds ratio is avoidable if we use an alternative aggregation to the standard arithmetic mean. We provide a new definition of collapsibility that makes this choice of aggregation method explicit, and we demonstrate that the odds ratio is collapsible under geometric aggregation. We describe how to partially identify, estimate, and do inference on the geometric odds ratio under outcome-dependent sampling. Our proposed estimator is based on the efficient influence function and therefore has doubly robust-style properties.
        </i></p>
        <script xml:space="preserve" language="JavaScript">
          hideblock('facct21_abs');
        </script>
        </div>
      </td>
    </tr>
   -->
</table><hr style="margin-top: 20px;">

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Visualization</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
  <tr width="90%" valign="center" align="left">
    <td style="padding-left: 4%;">
      <h3 style="margin-bottom: 6px">Opioid Epidemic</h3>
      In 2017, drug overdoses claimed more lives than car accidents. The below maps show opioid hotspots across the United States at various granularities. We used a similarity metric of death rate trajectories derived from Fisher's exact test to perform hierarchical clustering. <br><br>

      <center>
        <a href="start10.html" class="button" target="_blank">County Map</a>
        <a href="100clusters10.html" class="button" target="_blank">100 clusters</a>
        <a href="10clusters10.html" class="button" target="_blank">10 clusters</a>
      </center>
    </td>
  </tr>
</table>
<hr style="margin-top: 20px;"> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
  <tr width="90%" valign="center" align="left">
    <td style="padding-left: 3%;">
      Amanda particularly enjoys teaching and mentorship opportunties. She is the instructor for <a href="https://stat156.berkeley.edu/fall-2024/">Causal Inference</a> (STAT 156/256) at UC Berkeley. She served as a teaching assistant for Matt Gormley and Tom Mitchell's <a href="http://mlcourse.org">Introduction to Machine Learning</a> in 2021. She served as a project lead of the <a href="http://ai-4-all.org/">AI4ALL</a> summer program at CMU, where she introduced high school students to algorithmic fairness in the criminal justice system using the COMPAS dataset (see Github <a href= "https://github.com/mandycoston/compas-analysis">project</a>). As an undergraduate, she was a teaching assistant for Brian Kernighan's <a href="https://www.cs.princeton.edu/courses/archive/fall14/cos109/01intro.pdf">Computers in our World</a> course at Princeton.
    </td>
  </tr>
</table>
<hr style="margin-top: 20px;">

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Service</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5"> 
    <tr>
      <td width=30%" valign="top" align="right"><b><i>Referee</b></td>
      <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://www.nature.com/nathumbehav/">Nature Human Behaviour</a>.</td>
    </tr>

    <tr>
      <td width=30%" valign="top" align="right"><b><i>Referee</b></td>
      <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://hdsr.mitpress.mit.edu/">Harvard Data Science Review</a>.</td>
    </tr>

  <tr>
    <td width=30%" valign="top" align="right"><b><i>Referee</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://www.tandfonline.com/journals/uasa20">JASA</a>.</td>
  </tr>
  <tr>
    <td width=30%" valign="top" align="right"><b><i>Referee</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://jmlr.org/tmlr/">Transactions on Machine Learning Research</a>.</td>
  </tr>
<tr>
 <td width="30%" valign="top" align="right"><b><i>Referee</b></td>
 <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://rss.onlinelibrary.wiley.com/journal/14679868">Journal of Royal Statistical Society Series B</a>.</td>
</tr>
<tr>
  <td width="30%" valign="top" align="right"><b><i>Referee</b></td>
  <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://www.springer.com/journal/10618">Data Mining and Knowledge Discovery</a>.</td>
</tr>
<tr>
  <td width="30%" valign="top" align="right"><b><i>Steering Committee</b></td>
  <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://ml4d.notion.site/Machine-Learning-for-the-Developing-World-ML4D-2021-548251eab3df4517819c4742c2e5c853">ML4D workshop</a> 2022, 2021, 2020, 2019.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Reviewer</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://icml.cc/Conferences/">ICML</a> 2022, 2021, 2020, 2024.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Reviewer</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://iclr.cc/Conferences/2022/">ICLR</a> 2023, 2022.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Ethical reviewer</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://nips.cc/Conferences/2022">NeurIPS</a> 2022, 2021.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Reviewer</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://nips.cc/Conferences/2022">NeurIPS</a> 2022, 2021, 2020.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Reviewer</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks">NeurIPS Datasets and Benchmarks</a> 2022, 2021.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Program Committee</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://eaamo.org">EAAMO</a> 2022.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Program Committee</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://facctconference.org/">FAccT</a> 2022, 2021, 2020.</td>
  </tr>
  
  
  <!-- <tr>
    <td width="12%" valign="top" align="right"><b><i>2021</b></td>
    <td width="88%" valign="top" align="left" style="padding-left: 10px;">Programme committee member for <a href="https://facctconference.org/2021/">FAccT 2021</a>.</td>
  </tr> -->
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Area Chair</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://iclr.cc/virtual/2021/workshop/2132">ICLR Workshop on Responsible AI</a> 2021.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Program Commitee</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://www.aies-conference.com/2020/index.html">AIES</a> 2020.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Program Committee</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://aaai.org/Conferences/AAAI-20/aaai20specialtrackcall/">AAAI Emerging Track on AI for Social Impact</a> 2020.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Program Committee</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://aiforgood2019.github.io/">IJCAI Workshop on AI for Social Good</a> 2019.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Co-organizer</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;">Fairness, Ethics, Accountability, and Transparency (FEAT) reading group at CMU 2019-2020.</td>
  </tr>
  <tr>
    <td width="30%" valign="top" align="right"><b><i>Co-organizer</b></td>
    <td width="70%" valign="top" align="left" style="padding-left: 10px;"><a href="https://ml4d.notion.site/Machine-Learning-for-the-Developing-World-ML4D-2021-548251eab3df4517819c4742c2e5c853">ML4D workshop</a> at NeurIPS 2018 and NeurIPS 2019. ML4D showcases ML research by and for the developing world.</td>
  </tr>
</table>
<hr style="margin-top: 20px;">

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Background</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="5">
  <tr width="90%" valign="center" align="left">
    <td style="padding-left: 3%;">
      Amanda graduated from <a href="https://www.princeton.edu/">Princeton University</a> in 2013 with a degree in computer science and a certificate in the <a href="https://spia.princeton.edu/">Princeton School of Public Policy and International Affairs</a>. For her undergraduate thesis, she analyzed how machine learning techniques can improve the diagnosis of pediatric tuberculosis in collaboration with Jocelyn Tang ('14) and under the guidance of <a href="http://rob.schapire.net/">Robert Schapire</a>. In 2019 she earned her <a href="https://www.ml.cmu.edu/academics/secondary-ms.html">Master of Science in Machine Learning</a> from CMU.

    </td>
  </tr>
</table>
<hr style="margin-top: 20px;">

<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;Contact</sectionheading>
    <p style="margin-left: 10px;">
      Evans Hall<br>
      Room TBD<br>
      Department of Statistics<br>
      University of California, Berkeley<br>
      Berkeley, CA 94720
    </p>
  </td></tr>
</table><hr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><p align="right"><font size="1.5">
    Template modified from <a href="https://siddancha.github.io" target="_blank">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>

</html>
